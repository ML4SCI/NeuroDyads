{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12419330,"sourceType":"datasetVersion","datasetId":7832989}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install cebra ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:11:11.849029Z","iopub.execute_input":"2025-07-09T10:11:11.849249Z","iopub.status.idle":"2025-07-09T10:12:29.143873Z","shell.execute_reply.started":"2025-07-09T10:11:11.849231Z","shell.execute_reply":"2025-07-09T10:12:29.142852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport mne\nimport numpy as np\n\nRAW = Path(\"/kaggle/input/no-processing-run-nt9-10\")\nOUTPUT_DIR = Path(\"/kaggle/working/cebra_outputs\")\nOUTPUT_DIR.mkdir(exist_ok=True)\n\nPAIRINGS = {\n    \"lst9-spk10\": (\"nt9_listen.edf\", \"nt10_speak.edf\"),\n    \"spk9-lst10\": (\"nt9_speak.edf\", \"nt10_listen.edf\"),\n}\n\ndef load_eeg(path):\n    raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n    return raw.get_data(picks=\"eeg\")\n\ndef align_lengths(a, b):\n    T = min(a.shape[1], b.shape[1])\n    return a[:, :T], b[:, :T]\n\ndef minmax_per_channel(x):\n    xmin = x.min(axis=1, keepdims=True)\n    xmax = x.max(axis=1, keepdims=True)\n    rng  = np.where((xmax - xmin) == 0, 1, xmax - xmin)\n    return (x - xmin) / rng\n\nfor pair_name, (file1, file2) in PAIRINGS.items():\n    # Load EEG from both subjects\n    A = load_eeg(RAW / file1)\n    B = load_eeg(RAW / file2)\n    A, B = align_lengths(A, B)\n    stacked = np.vstack([A, B])  # shape: (channels, timepoints)\n\n    # Normalize channels independently\n    normalized = minmax_per_channel(stacked)\n\n    # Save normalized EEG data\n    npy_path = OUTPUT_DIR / f\"{pair_name}_normalized.npy\"\n    np.save(npy_path, normalized)\n    print(f\"âœ“ Saved: {npy_path.name}  {normalized.shape}\")\n\n    n_ch = A.shape[0]  # channels per subject\n    n_time = stacked.shape[1]  # number of timepoints\n\n    # Generate labels per channel (1D array length = channels)\n    if pair_name == \"lst9-spk10\":\n        # nt9 listens â†’ label 0, nt10 speaks â†’ label 1\n        channel_labels = np.array([0]*n_ch + [1]*n_ch)\n    elif pair_name == \"spk9-lst10\":\n        # nt9 speaks â†’ label 1, nt10 listens â†’ label 0\n        channel_labels = np.array([1]*n_ch + [0]*n_ch)\n    else:\n        raise ValueError(f\"Unknown pair_name {pair_name}\")\n\n    # Expand channel-wise labels to all timepoints: shape (channels, timepoints)\n    labels_2d = np.repeat(channel_labels[:, np.newaxis], n_time, axis=1)\n\n    # Save labels as 2D array (channels, timepoints)\n    np.save(OUTPUT_DIR / f\"{pair_name}_activity_labels.npy\", labels_2d)\n    print(f\"âœ“ Saved expanded activity labels for {pair_name} with shape {labels_2d.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:12:29.146118Z","iopub.execute_input":"2025-07-09T10:12:29.146369Z","iopub.status.idle":"2025-07-09T10:12:33.744338Z","shell.execute_reply.started":"2025-07-09T10:12:29.146344Z","shell.execute_reply":"2025-07-09T10:12:33.743608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#todo \n#think about what portion of data take for decoding: pos/neg. split into halfs, \n#grab second third of each part \n\n#number of the neighboors \n#see if that helps ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:12:33.745038Z","iopub.execute_input":"2025-07-09T10:12:33.745354Z","iopub.status.idle":"2025-07-09T10:12:33.748949Z","shell.execute_reply.started":"2025-07-09T10:12:33.745335Z","shell.execute_reply":"2025-07-09T10:12:33.748300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from cebra import CEBRA, KNNDecoder, plot_embedding, plot_loss\nfrom cebra.integrations.sklearn import metrics as cmetrics\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Output directory\nOUTPUT_DIR = Path(\"/kaggle/working/cebra_outputs\")\nOUTPUT_DIR.mkdir(exist_ok=True)\n\n# CEBRA configuration\nMODEL_KWARGS = dict(\n    model_architecture=\"offset10-model\",\n    batch_size=512,\n    learning_rate=3e-4,\n    temperature=1.12,\n    max_iterations=5000,\n    conditional=\"time_delta\",\n    output_dimension=3,\n    distance=\"cosine\",\n    device=\"cuda:0\",\n    verbose=True,\n    time_offsets=10,\n)\n\n# Max samples for faster decoding\nMAX_TRAIN_SAMPLES = 2000000\n\n\ndef run_cebra(pair_name, scale, run_id):\n    torch.manual_seed(run_id)\n    model = CEBRA(**MODEL_KWARGS)\n\n    # Load data\n    data_path = OUTPUT_DIR / f\"{pair_name}_normalized.npy\"\n    labels_path = OUTPUT_DIR / f\"{pair_name}_activity_labels.npy\"\n\n    normalized = np.load(data_path)  # (channels, timepoints)\n    activity_labels_2d = np.load(labels_path)  # (channels, timepoints)\n\n    channels, timepoints = normalized.shape\n\n    # Reshape to (timepoints * channels, 1)\n    X = normalized.T.reshape(-1, 1)\n    Y = activity_labels_2d.T.flatten()\n\n    # Train/test split\n    train_tp, test_tp = train_test_split(\n        np.arange(timepoints), test_size=0.2, random_state=42, shuffle=False\n    )\n    train_idx = np.hstack([np.arange(channels) + t * channels for t in train_tp])\n    test_idx = np.hstack([np.arange(channels) + t * channels for t in test_tp])\n\n    X_train, X_test = X[train_idx], X[test_idx]\n    Y_train, Y_test = Y[train_idx], Y[test_idx]\n\n        # Before training\n    print(\"ðŸ’¡ [Before Training]\")\n    print(\"  X shape:\", X_train.shape)  # should be (T, features)\n    print(\"  y shape:\", Y_train.shape)  # should be (T,)\n    print(\"  X dtype:\", X_train.dtype)\n    print(\"  y dtype:\", Y_train.dtype)\n    print(\"  Unique y labels:\", np.unique(Y_train))\n    print()\n\n\n    # Train CEBRA\n    model.fit(X_train, Y_train)\n    emb_train = model.transform(X_train)\n    emb_val = model.transform(X_test)\n    print('cebra trained')\n    print(\"  Embedding shape:\", emb_train.shape)  # (T, output_dim)\n\n\n    # Save model + embeddings\n    model.save(OUTPUT_DIR / f\"{pair_name}_{scale}_run{run_id}.pt\")\n    np.save(OUTPUT_DIR / f\"{pair_name}_{scale}_emb_train_run{run_id}.npy\", emb_train)\n    np.save(OUTPUT_DIR / f\"{pair_name}_{scale}_emb_val_run{run_id}.npy\", emb_val)\n\n    # Save plots\n    ax = plot_loss(model)\n    fig = ax.get_figure()\n    fig.savefig(OUTPUT_DIR / f\"{pair_name}_{scale}_loss_run{run_id}.png\")\n    plt.close(fig)\n\n    ax = plot_embedding(emb_val, embedding_labels=Y_test)\n    fig = ax.get_figure()\n    fig.savefig(OUTPUT_DIR / f\"{pair_name}_{scale}_embedding_run{run_id}.png\")\n    plt.close(fig)\n    print('plots done')\n\n    # Custom slicing: divide training embeddings into 6 equal parts\n    #total = emb_train.shape[0]\n    #sixth = total // 6\n\n    # Get 2nd and 4th parts\n    #part2_idx = slice(1 * sixth, 2 * sixth)\n    #part4_idx = slice(3 * sixth, 4 * sixth)\n\n    #emb_train_small = np.concatenate([emb_train[part2_idx], emb_train[part4_idx]], axis=0)\n    #Y_train_small = np.concatenate([Y_train[part2_idx], Y_train[part4_idx]], axis=0)\n\n    #print(f\"Using {emb_train_small.shape[0]} samples for decoding (2nd and 4th sixths)\")\n\n    print('decoding started')\n    decoder = KNNDecoder(n_neighbors=50000) # ask about the params \n    decoder.fit(emb_train, Y_train)\n    accuracy = decoder.score(emb_val, Y_test)\n    print('decoding finished')\n\n    # Goodness of fit\n    gof = cmetrics.goodness_of_fit_history(model)\n\n    # Save metrics\n    results = {\n        \"pair\": pair_name,\n        \"scale\": scale,\n        \"run_id\": run_id,\n        \"train_samples\": len(X_train),\n        \"val_samples\": len(X_test),\n        \"decoding_accuracy\": accuracy,\n        \"goodness_of_fit\": gof.tolist(),\n    }\n    with open(OUTPUT_DIR / f\"{pair_name}_{scale}_metrics_run{run_id}.json\", \"w\") as f:\n        json.dump(results, f, indent=2)\n\n    print(f\"âœ“ Done: {pair_name} [{scale}] run {run_id}\")\n\n\n# Run multiple experiments\nTO_RUN = {\n    \"lst9-spk10\": {\"normalized\": list(range(5))},\n    \"spk9-lst10\": {\"normalized\": list(range(5))},\n}\n\nsummary_rows = []\n\nfor pair_name, scales in TO_RUN.items():\n    for scale, run_ids in scales.items():\n        embeddings = []\n        labels = []\n\n        for run_id in run_ids:\n            run_cebra(pair_name, scale, run_id)\n\n            # Load embeddings and labels\n            emb_val = np.load(OUTPUT_DIR / f\"{pair_name}_{scale}_emb_val_run{run_id}.npy\")\n            activity_labels = np.load(OUTPUT_DIR / f\"{pair_name}_activity_labels.npy\").astype(np.int64)\n            n_samples = emb_val.shape[0]\n            repeated_labels = np.tile(activity_labels.T.flatten(), int(n_samples / activity_labels.size))\n            labels.append(repeated_labels)\n            embeddings.append(emb_val)\n\n            # Load accuracy\n            with open(OUTPUT_DIR / f\"{pair_name}_{scale}_metrics_run{run_id}.json\") as f:\n                acc_data = json.load(f)\n            summary_rows.append({\n                \"pair\": pair_name,\n                \"scale\": scale,\n                \"run\": run_id,\n                \"acc_train\": acc_data.get(\"train_accuracy\", np.nan),\n                \"acc_val\": acc_data.get(\"decoding_accuracy\", np.nan),\n            })\n\n        # Metrics across runs\n        consistency = cmetrics.consistency_score(embeddings, between=\"runs\")\n        dist_scores = cmetrics.intra_inter_class_distance(embeddings, labels)\n\n        with open(OUTPUT_DIR / f\"{pair_name}_{scale}_consistency.json\", \"w\") as f:\n            json.dump({\"consistency_score\": consistency}, f)\n\n        with open(OUTPUT_DIR / f\"{pair_name}_{scale}_class_distance.json\", \"w\") as f:\n            json.dump(dist_scores, f)\n\n# Save CSV summary\npd.DataFrame(summary_rows).to_csv(OUTPUT_DIR / \"decoder_accuracy_summary.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T10:13:15.049475Z","iopub.execute_input":"2025-07-09T10:13:15.049780Z","iopub.status.idle":"2025-07-09T10:15:00.376041Z","shell.execute_reply.started":"2025-07-09T10:13:15.049754Z","shell.execute_reply":"2025-07-09T10:15:00.374804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1620 neighboors as a square root of the 2626000 \n\n# later implement this in code \n# if it's fast enough try to do the whole dataset like this without cutting ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}